\documentclass[11pt]{article}
\usepackage{mathbbol}
\usepackage{amsmath}
\usepackage{amssymb}
\begin{document}
\noindent Elliot F. Poirier
\hfill
McGill University
\\
Summer 2024
\hfill
MATH470 - Honours Research Project
\\
\begin{center}
\textbf{\Large{Notes on Dantzig-Wolfe Decomposition}}
\end{center}
\hfill
\\
\noindent \textbf{\large{1. Block-Angular Structure}}
\\

\noindent A matrix $A$ has a \textbf{block angular structure} if it is of this form:
\[
A = \begin{bmatrix}
A_{1} & A_{2} & \cdots & A_{n-1} & A_{n} \\
A_{n+1} & 0 & \cdots & 0 & 0 \\
0 & A_{n+2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 0 & A_{2n}
\end{bmatrix}
\]

\noindent where $A_{i}$ are submatrices of size $m_i \times n_i$. We call the first $n$ matrices $A_1, A_2, \cdots, A_{n-1}$ the bounding constraints and the last $n$ matrices the independent constraints. Notice that this easily becomes a linear program that looks like this, 

\begin{aligned}
\text{maximize} \quad & \sum_{i=1}^{n} \vec{c}_i \vec{x}_i \\
\text{subject to} \quad & [A_1, \cdots, A_n, I] \vec{x}^T = \vec{b}_0 \quad \text{  linking constraints}\\
& A_{n+i} \vec{x}_i = \vec{b}_i, \quad \forall i \in \{1, \cdots, n\} \quad \text{  independent constraints} \\
& \vec{x}^T \geq 0, \quad \vec{x}_i \geq 0, \quad \forall i \in \{1, \cdots, n\} \quad \text{  non-negativity constraints}
\end{aligned}
\\
\\

\noindent In this linear program, the $\vec{c}_i$ are the costs and $\vec{x} = [\vec{x}_1, \cdots, \vec{x}_n]^T$ are the decision variables and $\vec{b}_i$ are the right hand side of the constraints. The aim of the Dantzig-Wolfe decomposition is to be more computationally efficient in solving this problem. We start with a \textbf{restricted master problem} (RMP), which usually encompasses all the linking constraints and then check the independent constraints one by one by solving them as subproblems.

\newpage

\noindent \textbf{\large{2. Dantzig-Wolfe Decomposition}}
\\

\noindent \textbf{2.1 Intro}
\\

\noindent We'll start by giving a high level view of the algorithm, which is basically doing these steps (Wikipedia):
\begin{enumerate}
    \item Start with a \underline{feasible} solution to the RMP, formulate new objective functions for each subproblem such that the subproblems will offer solutions that improve the current objective of the master program.
    \item Re-solve the subproblems given their new objective functions, the optimal value for each subproblem is offered to the master program.
    \item Incorporate one or all of the new columns generated by the solutions to the subproblems into the master program based on their respective ability to improve the original problem's objective.
    \item Perform \(x\) iterations of the simplex algorithm in the master program, where \(x\) is the number of columns incorporated.
    \item If the objective is improved, go to step 1. Otherwise, we ran out of improvement so we return the solution.
\end{enumerate}

\noindent Note that Dantzig-Wolfe is most effective when the subproblems are easy to solve. Now, let's break it down into more detail.  The first step consists of solving the RMP and then defining the objective functions. The RMP looks like this,

\begin{aligned}
    \text{maximize} \quad & \sum_{i=1}^{n} \vec{c}_i \vec{x}_i \\
    \text{subject to} \quad & [A_1, \cdots, A_n, I] \vec{x}^T = \vec{b}_0  \\
    & \vec{x}^T \geq 0\\
\end{aligned}
\\
\\

\noindent The subproblems look like this, for every $i$th subproblem,

\begin{aligned}
    \text{maximize} \quad & \vec{c}_i \vec{x}_i \\
    \text{subject to} \quad & A_{n+i} \vec{x}_i = \vec{b}_i \\
    & \vec{x}_i \geq 0
\end{aligned}
\\

\noindent What interests us is the feasible region of the subproblems, we will denote them as $S_i = \{\vec{x}_i \geq 0 | A_{n+i} \vec{x}_i = \vec{b}_i\}$. Assume that $S_i$ is bounded for simplicity (it can easily be extended to having unbounded regions, we would consider unbounded rays). We have a set of vertices $V_i = \{(v_{i,1}, \cdots, v_{i,n}) | v_{i,j} \in S_i\}$ which completely defines the feasible region of the subproblem. This implies that we can represent every $x_i \in S_i$ as a convex combination of the vertices $v_{i,j}$, i.e. $\exists \{\lambda_{i,j} \geq 0\}_j \text{ s.t. } x_i = \sum_{j=1}^{n} \lambda_{i,j} v_{i,j}$ and with $\sum_{j=1}^{n} \lambda_{i,j} = 1$ (this follows from the representation theorem).
\\

\noindent Now, we can transform the RMP by using the $\lambda_{i,j}$ as the new decision variables.
\\

\begin{array}{clll}
    \min & \sum_{i=1}^m \sum_{j=1}^{N_i} \lambda_{i, j}\left(c_i v_{i, j}\right) & & \\
    \mathrm{s.t.} & \sum_{i=1}^m \sum_{j=1}^{N_i} \lambda_{i, j}\left(A_{0, i} v_{i, j}\right) & =b_0 & \\
    & \sum_{j=1}^{N_i} \lambda_{i, j} & =1 & \forall i=1,2, \ldots, m \\
    & \lambda_{i, j} & \geq 0 & \forall i, j
\end{array}\\

\noindent Notably, we reduced the number of constraints to $m + m_0$, since there aren now $m$ independent constraints and $m_0$ linking constraints. Basically, we squashed all the independent constraint blocks into single constraints. However, there are now way more variables, which is why we will use an altered simplex method.
\\

\noindent \textbf{2.2 Altered Simplex}
\\

\noindent We start by creating the dual variables, $y_0 \in \mathbb{R}^{m_0}$ and $z \in \mathbb{R}^{m}$ which set the sum $\sum_{i=1}^{m} \lambda_{i, j} =1$. Note that each column represents one vertex. And so, we want to find out which columns have negative reduced cost. If we consider the column for $\lambda_{i, j}$, then the reduced cost is 
$$ \bar{c}_i = c_i v_{i, j} - y_0 A_{0, i} v_{i, j} - z_i $$

\noindent which we can obtain by taking the inner product of the dual variables with the column for $\lambda_{i, j}$.

$$\left[\begin{array}{ll}
    y_0 & z 
    \end{array} \right]\left[\frac{A_{0 i} v_{i j}}{c_i}\right] \quad \text{(unsure about this)}
    $$

\noindent So now we want to know if there exists $v_{i, j}$ such that $\bar{c}_i < 0$ or equivalently, $c_i v_{i, j} - y_0 A_{0, i} v_{i, j} - z_i < 0$. If so, we can then find the column for $\lambda_{i, j}$ that has the smallest reduced cost? This means we want to find a vertex of $S_i$ such that $(c_i - y_0 A_{0, i}) v_{i, j} - z_i < 0$.
\\

\noindent We now consider the resulting subproblem,
\\

\begin{array}{rcl}
    \min & \bar{c}_i x_i & \\
    \text { s.t. } & A_{n+i} x_i & =b_i, \\
    & x_i & \geq 0
\end{array}
\\

\noindent Since $S_i$ is nonempty (otherwise the master problem is infeasible), this problem must contain a vertex $\textbf{v}$ which is the optimal solution. If $\bar{c}_i\textbf{v}<z_i$ then we know that $\textbf{v}$ is a negative cost vertex and we should add it to the basis. If this is false $\forall i$, then no column of $S_i$ has negative reduced cost. Hence, after solving the subproblems for all $S_i$, we either find a negative reduced cost column or otherwise we show that the current solution to the master problem is optimal.
\\

\noindent Notice that this in itself is fundamentally a revised version of the simplex algorithm, however with significantly reduced usage of space. Additionally, it is more efficient computationally since the slave problems can be solved in parallel (that is the big advantage), we simply choose the first negative reduced cost column subproblem to be computed.
\\

\noindent \textbf{2.3 Examples}
\\

\noindent Our particular interest with this algorithm stems from the particular structure of the optimization problem we are trying to solve. Let us formalize the problem of block allocation (mining blocks) in the context of mine planning. Our particular problem is of this form, 
\\

$\begin{aligned} 
    \max & \sum_{b \in B}\left(V_{b, A, I} m_{b, A, I}+V_{b, A, II} m_{b, A, II}+V_{b, B, I} m_{b, B, I}+V_{b, B, II} m_{b, B, II}\right) \\ 
    \text { st. } & \sum_{b \in B B}\left(\frac{m_{b, A, I}}{r_{A, I}}+\frac{m_{b, A, II}}{r_{A, II}}+\frac{m_{b, B, I}}{r_{B, I}}+\frac{m_{b, B, II}}{r_{B, II}}\right) \leqslant t_{\text {period }} \text { (1) } \\ 
    & m_{b, A, I}+m_{b, B, I} \leqslant m_{b, I}  \\ 
    & m_{b, A, I}+m_{b, B, I} \leqslant m_{b, I}  \quad \forall b \in \mathcal{B} \quad(3) \\ 
    & m_{i j k}, r_{j k} \geqslant 0 \quad \forall i \in \mathcal{B}, j \in \text { modes, } k \in \text { ores }
\end{aligned}$
\\

\noindent With (1) being that total processing time cannot be greater than the total period & (2) being that the total amount of ore being processed in a block cannot exceed the capacity of the block. It is to be noted that we are missing a constraint related to the rate of extraction.
\\

\noindent Now, we will look at two examples to illustrate this, both with two modes (A, B) and two ore types (I, II), one with one block, and one with two blocks. 
\\

\noindent \textbf{2.3.1} With one block, we have this problem, 
\\

$\begin{aligned} 
    \max & \left(V_{A, I} m_{A, I}+V_{A, II} m_{A, II}+V_{B, I} m_{B, I}+V_{B, II} m_{B, II}\right) \\ 
    \text { st. } &   \frac{m_{A, I}}{r_{A, I}}+\frac{m_{A, II}}{r_{A, II}}+\frac{m_{B, I}}{r_{B, I}}+\frac{m_{B, II}}{r_{B, II}} \leqslant t_{\text {period }} \text { (1) } \\ 
    & m_{A, I}+m_{B, I} \leqslant m_{I}  \\ 
    & m_{A, I}+m_{B, I} \leqslant m_{I}  \\ 
    & m_{A, I}, m_{A, II}, m_{B, I}, m_{B, II}, r_{A, I}, r_{A, II}, r_{B, I}, r_{B, II} \geqslant 0
\end{aligned}$
\\

\noindent So, now our first step, 

\noindent \textbf{2.3.2} With two blocks, we have this problem, 
\\

$\begin{aligned} 
    \max & \left(V_{1,A, I} m_{1,A, I}+V_{1,A, II} m_{1,A, II}+V_{1,B, I} m_{1,B, I}+V_{1,B, II} m_{1,B, II}\right) + \left(V_{2,A, I} m_{2,A, I}+V_{2,A, II} m_{2,A, II}+V_{2,B, I} m_{2,B, I}+V_{2,B, II} m_{2,B, II}\right) \\ 
    \text { st. } &   \left(\frac{m_{1,A, I}}{r_{1,A, I}}+\frac{m_{1,A, II}}{r_{1,A, II}}+\frac{m_{1,B, I}}{r_{1,B, I}}+\frac{m_{1,B, II}}{r_{1,B, II}}\right) + \left(\frac{m_{2,A, I}}{r_{2,A, I}}+\frac{m_{2,A, II}}{r_{2,A, II}}+\frac{m_{2,B, I}}{r_{2,B, I}}+\frac{m_{2,B, II}}{r_{2,B, II}}\right) \leqslant t_{\text {period }} \text { (1) } \\ 
    & m_{1,A, I}+m_{1,B, I} \leqslant m_{I}  \\ 
    & m_{1,A, II}+m_{1,B, II} \leqslant m_{II}  \\ 
    & m_{2,A, I}+m_{2,B, I} \leqslant m_{I}  \\ 
    & m_{2,A, II}+m_{2,B, II} \leqslant m_{II}  \\ 
    & m_{i,j,k}, r_{j,k} \geqslant 0 \quad \forall i \in \{1,2\} \quad j \in \{A,B\} \quad k \in \{I,II\}
\end{aligned}$





\end{document}